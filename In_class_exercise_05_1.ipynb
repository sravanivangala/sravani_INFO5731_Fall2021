{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "In_class_exercise_05-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanivangala/sravani_INFO5731_Fall2021/blob/main/In_class_exercise_05_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ObAZzDZemJn"
      },
      "source": [
        "# **The fifth in-class-exercise (40 points in total, 11/11/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN-PrzDdemJv"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
        "\n",
        "The dataset can be download from here: https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/exercise09_datacollection.zip. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
        "\n",
        "Algorithms:\n",
        "\n",
        "(1) MultinominalNB\n",
        "\n",
        "(2) SVM \n",
        "\n",
        "(3) KNN \n",
        "\n",
        "(4) Decision tree\n",
        "\n",
        "(5) Random Forest\n",
        "\n",
        "(6) XGBoost\n",
        "\n",
        "Evaluation measurement:\n",
        "\n",
        "(1) Accuracy\n",
        "\n",
        "(2) Recall\n",
        "\n",
        "(3) Precison \n",
        "\n",
        "(4) F-1 score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1-0wtqfemJw"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold, KFold, cross_val_score\n",
        "from xgboost import XGBClassifier\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "uYwtTUuu3V5Q",
        "outputId": "de006c16-6b5c-48d8-8093-e437f204b93c"
      },
      "source": [
        "train_df = pd.read_csv(r'/content/sample_data/stsa-train.txt',sep = 'delimiter=',header= None,names=['Text'])\n",
        "train_df[['Sentiment','Text']] = train_df[\"Text\"].str.split(\" \", 1, expand=True)\n",
        "test_df = pd.read_csv(r'/content/sample_data/stsa-test.txt',sep = 'delimiter=',header= None,names=['Text'])\n",
        "test_df[['Sentiment','Text']] = test_df[\"Text\"].str.split(\" \", 1, expand=True)\n",
        "train_df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting-room f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text Sentiment\n",
              "0  a stirring , funny and finally transporting re...         1\n",
              "1  apparently reassembled from the cutting-room f...         0\n",
              "2  they presume their audience wo n't sit still f...         0\n",
              "3  this is a visually stunning rumination on love...         1\n",
              "4  jonathan parker 's bartleby should have been t...         1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "8IFBem_E3bN5",
        "outputId": "9ac26a7a-6ab2-475a-fa5a-710d7d928076"
      },
      "source": [
        "\n",
        "test_df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>no movement , no yuks , not much of anything .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we never really feel involved with the story ,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is one of polanski 's best films .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text Sentiment\n",
              "0     no movement , no yuks , not much of anything .         0\n",
              "1  a gob of drivel so sickly sweet , even the eag...         0\n",
              "2  gangs of new york is an unapologetic mess , wh...         0\n",
              "3  we never really feel involved with the story ,...         0\n",
              "4            this is one of polanski 's best films .         1"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcd7XHuj3fma",
        "outputId": "80633c7e-d6da-4f54-e274-517805fe7ca4"
      },
      "source": [
        "# Clean data\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopword=nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl= WordNetLemmatizer()\n",
        "def clean_text(data):\n",
        "  data=\"\".join([word.lower() for word in data if word not in string.punctuation])\n",
        "  data = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", data)\n",
        "  tokens = re.split('\\W+',data)\n",
        "  data = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
        "  return data"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M0_b-XR3nzS",
        "outputId": "e7436f0a-366c-4dce-c85e-a7dee6d1780b"
      },
      "source": [
        "# Convert text and train data into numerical\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vect = TfidfVectorizer(analyzer = clean_text)\n",
        "X_tfidf = tfidf_vect.fit_transform(train_df['Text'])\n",
        "print(X_tfidf.shape)\n",
        "X_tfidf_df=pd.DataFrame(X_tfidf.toarray())\n",
        "X_tfidf_df.columns=tfidf_vect.get_feature_names()\n",
        "X_test_tfidf = tfidf_vect.transform(test_df['Text'])\n",
        "print(X_test_tfidf.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 13343)\n",
            "(1821, 13343)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX7c2ojO3sPC"
      },
      "source": [
        "#Algorithms\n",
        "#(1) MultinominalNB\n",
        "mnb = MultinomialNB()\n",
        "#(2) SVM\n",
        "svm = LinearSVC()\n",
        "#(3) KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n",
        "#(4) Decision tree\n",
        "dt = DecisionTreeClassifier()\n",
        "#(5) Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "#(6) XGBoost\n",
        "xgb = XGBClassifier()\n",
        "\n",
        "#split the data\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_df['Sentiment'].values,\n",
        "                                                test_size=0.2, random_state=42)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_z2CX-y3ws3"
      },
      "source": [
        "#Defining method for cross validation score for all models\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "def cross_validation_score(modelName, x, y):\n",
        "  scoring = 'accuracy'\n",
        "  kfold = KFold(10, random_state = 7,shuffle=True)\n",
        "  cross_val = cross_val_score(modelName, x, y, cv=kfold).mean()\n",
        "  return cross_val"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvlTfd9G30_e",
        "outputId": "376d7b12-c951-4381-de77-25fda20aec02"
      },
      "source": [
        "\n",
        "#(1) MultinominalNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "mnb_test = mnb.fit(x_train,y_train)\n",
        "y_mnb = mnb_test.predict(x_test)\n",
        "print('Accuracy Score %s' % accuracy_score(y_mnb,y_test))\n",
        "print(classification_report(y_test,y_mnb))\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores_MNB = cross_val_score(mnb, x_test, y_test, cv=10)\n",
        "kfold = KFold(10, random_state = 7, shuffle=True)\n",
        "print(\"Accuracy with MNB\",scores_MNB.mean())\n",
        "mnb_scores_mean = cross_validation_score(mnb,x_test,y_test)\n",
        "print('MNB Cross Validation Score is {0}'.format(mnb_scores_mean))\n",
        "test_predicted_mnb = mnb.predict(x_test)\n",
        "test_accuracy_mnb = round(accuracy_score(test_predicted_mnb, y_test),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_mnb)\n",
        "print(classification_report(y_test, test_predicted_mnb))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score 0.7955202312138728\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.70      0.77       671\n",
            "           1       0.76      0.88      0.82       713\n",
            "\n",
            "    accuracy                           0.80      1384\n",
            "   macro avg       0.80      0.79      0.79      1384\n",
            "weighted avg       0.80      0.80      0.79      1384\n",
            "\n",
            "Accuracy with MNB 0.7247054530288813\n",
            "MNB Cross Validation Score is 0.7138828067980398\n",
            "Testing Data\n",
            "Accuracy -  79.55\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.70      0.77       671\n",
            "           1       0.76      0.88      0.82       713\n",
            "\n",
            "    accuracy                           0.80      1384\n",
            "   macro avg       0.80      0.79      0.79      1384\n",
            "weighted avg       0.80      0.80      0.79      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGUGwtiJ35Mz",
        "outputId": "9d1850f5-1b03-4600-9aee-4d8ab07f16b4"
      },
      "source": [
        "#(2) SVM\n",
        "svm_test = svm.fit(x_train,y_train)\n",
        "y_svm = svm_test.predict(x_test)\n",
        "print('Accuracy Score %s' % accuracy_score(y_svm,y_test))\n",
        "print(classification_report(y_test,y_svm))\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores_SVM = cross_val_score(svm, x_test, y_test, cv=10)\n",
        "print(\"Accuracy with SVM\",scores_SVM.mean())\n",
        "svm_scores_mean = cross_validation_score(svm, x_test,y_test)\n",
        "print('SVM Cross Validation Score is {0}'.format(svm_scores_mean))\n",
        "test_predicted_svm = svm.predict(x_test)\n",
        "test_accuracy_svm = round(accuracy_score(test_predicted_svm, y_test),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_svm)\n",
        "print(classification_report(y_test, test_predicted_svm))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score 0.791907514450867\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       671\n",
            "           1       0.78      0.83      0.80       713\n",
            "\n",
            "    accuracy                           0.79      1384\n",
            "   macro avg       0.79      0.79      0.79      1384\n",
            "weighted avg       0.79      0.79      0.79      1384\n",
            "\n",
            "Accuracy with SVM 0.7348034615785632\n",
            "SVM Cross Validation Score is 0.7348451673443853\n",
            "Testing Data\n",
            "Accuracy -  79.19\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.75      0.78       671\n",
            "           1       0.78      0.83      0.80       713\n",
            "\n",
            "    accuracy                           0.79      1384\n",
            "   macro avg       0.79      0.79      0.79      1384\n",
            "weighted avg       0.79      0.79      0.79      1384\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfOdNjIc389W"
      },
      "source": [
        "#KNN Model\n",
        "knn_test = knn.fit(x_train,y_train)\n",
        "y_knn = knn_test.predict(x_test)\n",
        "print('Accuracy Score %s' % accuracy_score(y_knn,y_test))\n",
        "print(classification_report(y_test,y_knn))\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores_KNN = cross_val_score(knn, x_test, y_test, cv=10)\n",
        "print(\"Accuracy with knn\",scores_KNN.mean())\n",
        "knn_scores_mean = cross_validation_score(knn, x_test,y_test)\n",
        "print('KNN Cross Validation Score is {0}'.format(knn_scores_mean))\n",
        "test_predicted_knn = knn.predict(x_test)\n",
        "test_accuracy_knn = round(accuracy_score(test_predicted_knn, y_test),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_knn)\n",
        "print(classification_report(y_test, test_predicted_knn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es2u66Ny4Elj"
      },
      "source": [
        "#(4) Decision tree\n",
        "des_tree_test = dt.fit(x_train,y_train)\n",
        "y_des_tree = des_tree_test.predict(x_test)\n",
        "print('Accuracy Score %s' % accuracy_score(y_des_tree,y_test))\n",
        "print(classification_report(y_test,y_des_tree))\n",
        "scores_DT = cross_val_score(dt, x_test, y_test, cv=10)\n",
        "print(\"Accuracy with Decision trees\",scores_DT.mean())\n",
        "dt_scores_mean = cross_validation_score(dt, x_test,y_test)\n",
        "print('Decision Tree Cross Validation Score is {0}'.format(dt_scores_mean))\n",
        "test_predicted_dt = dt.predict(x_test)\n",
        "test_accuracy_dt = round(accuracy_score(test_predicted_dt, y_test),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_dt)\n",
        "print(classification_report(y_test, test_predicted_dt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2sBDrnv4IQ5"
      },
      "source": [
        "#(5) Random Forest\n",
        "rf_test = rf.fit(x_train,y_train)\n",
        "y_rf = rf_test.predict(x_test)\n",
        "print('Accuracy Score %s' % accuracy_score(y_rf,y_test))\n",
        "print(classification_report(y_test,y_rf))\n",
        "scores_RF = cross_val_score(rf, x_test, y_test, cv=10)\n",
        "print(\"Accuracy with Random Forest\",scores_RF.mean())\n",
        "rf_scores_mean = cross_validation_score(rf, x_test,y_test)\n",
        "print('Random Forest Cross Validation Score is {0}'.format(rf_scores_mean))\n",
        "test_predicted_rf = rf.predict(x_test)\n",
        "test_accuracy_rf = round(accuracy_score(test_predicted_rf, y_test),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_rf)\n",
        "print(classification_report(y_test, test_predicted_rf))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgHjooRp4MHA"
      },
      "source": [
        "#(6) XGBoost\n",
        "xgb_test = xgb.fit(x_train,y_train)\n",
        "y_xgb = xgb_test.predict(x_test)\n",
        "print('Accuracy Score %s' % accuracy_score(y_xgb,y_test))\n",
        "print(classification_report(y_test,y_xgb))\n",
        "scores_XGB = cross_val_score(xgb, x_test, y_test, cv=10)\n",
        "print(\"Accuracy with XGBoost\",scores_XGB.mean())\n",
        "xgb_scores_mean = cross_validation_score(xgb, x_test,y_test)\n",
        "print('XGBoost Cross Validation Score is {0}'.format(xgb_scores_mean))\n",
        "test_predicted_xgb = xgb.predict(x_test)\n",
        "test_accuracy_xgb = round(accuracy_score(test_predicted_xgb, y_test),4)*100\n",
        "print(\"Testing Data\")\n",
        "print(\"Accuracy - \", test_accuracy_xgb)\n",
        "print(classification_report(y_test, test_predicted_xgb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeyNYbnW4QJy"
      },
      "source": [
        "print(\"Accuracy with MNB\",scores_MNB.mean())\n",
        "print(\"Accuracy with SVM\",scores_SVM.mean())\n",
        "print(\"Accuracy with knn\",scores_KNN.mean())\n",
        "print(\"Accuracy with Decision trees\",scores_DT.mean())\n",
        "print(\"Accuracy with Random Forest\",scores_RF.mean())\n",
        "print(\"Accuracy with XGBoost\",scores_XGB.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eEolkjS4Uu0"
      },
      "source": [
        "predict_svm = svm_test.predict(X_test_tfidf)\n",
        "print('Final trained model(SVM) with high accuracy evaluated on the test data: %s' % accuracy_score(predict_svm,test_df['Sentiment']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbH7xGyIemJx"
      },
      "source": [
        "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering\n",
        "Please downlad the dataset by using the following link.  https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
        "(You can also use different text data which you want)\n",
        "\n",
        "Apply the listed clustering methods to the dataset:\n",
        "\n",
        "K means, \n",
        "DBSCAN,\n",
        "Hierarchical clustering. \n",
        "\n",
        "You can refer to of the codes from  the follwing link below. \n",
        "https://www.kaggle.com/karthik3890/text-clustering "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2egA7Ur8emJy"
      },
      "source": [
        "#Write your code here.\n",
        "import pandas as pd\n",
        "#load the data\n",
        "sample=pd.read_csv(\"/content/sample_data/Amazon_Unlocked_Mobile.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B7rz7yLvUNb"
      },
      "source": [
        "print(sample.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uO46OjgHvaz8"
      },
      "source": [
        "sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1FxeP3Cvk4q"
      },
      "source": [
        "# Understand how customer ratings are distributed\n",
        "import seaborn as sns\n",
        "sns.countplot(sample.Rating)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTSXjCkEwAdY"
      },
      "source": [
        "#converting the Numerical reviws to categorical reviews on codition above 3 are\n",
        "#positive and below 3 are negative as reviews rating with 3 are not much useful\n",
        "#for analysis\n",
        "\n",
        "#function\n",
        "def partition(x):\n",
        "    if x < 3:\n",
        "        return 'negative'\n",
        "    return 'positive'\n",
        "\n",
        "#changing reviews with score less than 3 to be positive\n",
        "actualScore = sample['Rating']\n",
        "positiveNegative = actualScore.map(partition) \n",
        "sample['Rating'] = positiveNegative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2CujW71wcEl"
      },
      "source": [
        "sample.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vO1cs4Zwr3o"
      },
      "source": [
        "# no of positive and negative reviews\n",
        "sample[\"Rating\"].value_counts()\n",
        "#here we can say it is a unbalanced data set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhcnb2hIw0bX"
      },
      "source": [
        "#dropping  the duplicates column if any using drop duplicates from pandas\n",
        "#sorted_data=sample.sort_values('Reviews', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n",
        "#final=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\n",
        "#final.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzuV2MZbLc_6"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(sample.Rating)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuJ1GMBVND-p"
      },
      "source": [
        "# find sentences containing HTML tags\n",
        "import re\n",
        "i=0;\n",
        "for sent in sample['Reviews'].values:\n",
        "    if (len(re.findall('<.*?>', sent))):\n",
        "        print(i)\n",
        "        print(sent)\n",
        "        break;\n",
        "    i += 1;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XyomQ1AN38q"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer which is developed in recent years\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', sentence)\n",
        "    return cleantext\n",
        "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    return  cleaned\n",
        "print(stop)\n",
        "print('************************************')\n",
        "print(sno.stem('tasty'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp26KBi5u530"
      },
      "source": [
        "sample['Reviews']=sample['Reviews'].fillna(\"NA\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7GP5hjJOYSf"
      },
      "source": [
        "i=0\n",
        "str1=' '\n",
        "final_string=[]\n",
        "all_positive_words=[] # store words from +ve reviews here\n",
        "all_negative_words=[] # store words from -ve reviews here.\n",
        "s=''\n",
        "for sent in sample['Reviews'].values:\n",
        "    filtered_sentence=[]\n",
        "    #print(sent);\n",
        "    #sent=cleanhtml(sent) # remove HTMl tags\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
        "                if(cleaned_words.lower() not in stop):\n",
        "                    s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
        "                    filtered_sentence.append(s)\n",
        "                    if (sample['Rating'].values)[i] == 'positive': \n",
        "                        all_positive_words.append(s) #list of all words used to describe positive reviews\n",
        "                    if(sample['Rating'].values)[i] == 'negative':\n",
        "                        all_negative_words.append(s) #list of all words used to describe negative reviews reviews\n",
        "                else:\n",
        "                    continue\n",
        "            else:\n",
        "                continue \n",
        "    #print(filtered_sentence)\n",
        "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
        "    #print(\"***********************************************************************\")\n",
        "    \n",
        "    final_string.append(str1)\n",
        "    i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5iwjfpXwLOf"
      },
      "source": [
        "sample['CleanedText']=final_string #adding a column of CleanedText which displays the data after pre-processing of the review \n",
        "sample['CleanedText']=sample['CleanedText'].str.decode(\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6JTo3zkwjxt"
      },
      "source": [
        "sample.shape # cleaned text column added"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL8RT1_DwsLr"
      },
      "source": [
        "sample.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNt5N_qpxSYO"
      },
      "source": [
        "K means using bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCZOBtsSw4wZ"
      },
      "source": [
        "# Generating bag of words features.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "bow = count_vect.fit_transform(sample['CleanedText'].values)\n",
        "bow.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBu_cL-ux4YD"
      },
      "source": [
        "bow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvcNnkQ3x6gS"
      },
      "source": [
        "# to understand what kind of words generated as columns by BOW\n",
        "terms = count_vect.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmHtTJC3yBNW"
      },
      "source": [
        "#first 10 columns generated by BOW\n",
        "terms[1:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWlqfJY2ySjz"
      },
      "source": [
        "#using all processes jobs=-1 and k means++ for starting initilization advantage\n",
        "from sklearn.cluster import KMeans\n",
        "model = KMeans(n_clusters = 10,init='k-means++', n_jobs = -1,random_state=99)\n",
        "model.fit(bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ql31MPPzH2L"
      },
      "source": [
        "labels = model.labels_\n",
        "cluster_center=model.cluster_centers_\n",
        "\n",
        "cluster_center"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFYKxiq5jxUj"
      },
      "source": [
        "from sklearn import metrics\n",
        "silhouette_score = metrics.silhouette_score(bow, labels, metric='euclidean')\n",
        "# which tells us that clusters are far away from each other \n",
        "silhouette_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGRAOvBfk8G7"
      },
      "source": [
        "# Giving Labels/assigning a cluster to each point/text \n",
        "df = sample\n",
        "df['Bow Clus Label'] = model.labels_ # the last column you can see the label numebers\n",
        "df.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfHaeFN-oOU2"
      },
      "source": [
        "# How many points belong to each cluster -> using group by in pandas\n",
        "df.groupby(['Bow Clus Label'])['Reviews'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnyjJoAyoSR7"
      },
      "source": [
        "print(\"Top terms per cluster:\")\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = count_vect.get_feature_names()\n",
        "for i in range(10):\n",
        "    print(\"Cluster %d:\" % i, end='')\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms[ind], end='')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq3LRMONoXtY"
      },
      "source": [
        "# visually how points or reviews are distributed across 10 clusters \n",
        "import matplotlib.pyplot as plt\n",
        "plt.bar([x for x in range(10)], df.groupby(['Bow Clus Label'])['Reviews'].count(), alpha = 0.4)\n",
        "plt.title('KMeans cluster points')\n",
        "plt.xlabel(\"Cluster number\")\n",
        "plt.ylabel(\"Number of reviews\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdXTTFgaocic"
      },
      "source": [
        "# Reading a review which belong to each group.\n",
        "for i in range(10):\n",
        "    print(\"A review assigned to cluster \", i)\n",
        "    print(\"-\" * 70)\n",
        "    print(df.iloc[df.groupby(['Bow Clus Label']).groups[i][0]]['Reviews'])\n",
        "    print('\\n')\n",
        "    print(\"_\" * 70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRdh2aXJoiqG"
      },
      "source": [
        "#considers sample of 3 random reviews for cluster 0\n",
        "\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[0][3]]['Reviews'])\n",
        "print(\"_\" * 70)\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[0][15]]['Reviews'])\n",
        "print(\"_\" * 70)\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[0][25]]['Reviews'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgOTvzRDonm6"
      },
      "source": [
        "#consider sample of 3 random reviews for cluster 3\n",
        "\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[3][3]]['Reviews'])\n",
        "print(\"_\" * 70)\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[3][15]]['Reviews'])\n",
        "print(\"_\" * 70)\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[3][25]]['Reviews'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXoqIgeror4U"
      },
      "source": [
        "#consider sample of 3 random reviews for cluster 5\n",
        "\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[5][3]]['Reviews'])\n",
        "print(\"_\" * 70)\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[5][15]]['Reviews'])\n",
        "print(\"_\" * 70)\n",
        "print(df.iloc[df.groupby(['Bow Clus Label']).groups[5][25]]['Reviews'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmmLqQh5ov-s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhnL_CSimdjr"
      },
      "source": [
        "Clustering DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_pgYmRrBUWz"
      },
      "source": [
        "i=0\n",
        "list_of_sent_train=[]\n",
        "for sent in sample['CleanedText'].values:\n",
        "    filtered_sentence=[]\n",
        "    sent=cleanhtml(sent)\n",
        "    for w in sent.split():\n",
        "        for cleaned_words in cleanpunc(w).split():\n",
        "            if(cleaned_words.isalpha()):    \n",
        "                filtered_sentence.append(cleaned_words.lower())\n",
        "            else:\n",
        "                continue \n",
        "    list_of_sent_train.append(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTVH97GCBbf5"
      },
      "source": [
        "import numpy as np\n",
        "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this train\n",
        "for sent in list_of_sent_train: # for each review/sentence\n",
        "    sent_vec = np.zeros(100) # as word vectors are of zero length\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
        "    for word in sent: # for each word in a review/sentence\n",
        "        try:\n",
        "            vec = w2v_model.wv[word]\n",
        "            sent_vec += vec\n",
        "            cnt_words += 1\n",
        "        except:\n",
        "            pass\n",
        "    sent_vec /= cnt_words\n",
        "    sent_vectors.append(sent_vec)\n",
        "sent_vectors = np.array(sent_vectors)\n",
        "sent_vectors = np.nan_to_num(sent_vectors)\n",
        "sent_vectors.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apYiqjiYmeEs"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Computing 200th Nearest neighbour distance\n",
        "minPts = 2 * 100\n",
        "\n",
        "def lower_bound(nums, target): # This function return the number in the array just greater than or equal to itself.\n",
        "    l, r = 0, len(nums) - 1\n",
        "    while l <= r: # Binary searching.\n",
        "        mid = int(l + (r - l) / 2)\n",
        "        if nums[mid] >= target:\n",
        "            r = mid - 1\n",
        "        else:\n",
        "            l = mid + 1\n",
        "    return l\n",
        "\n",
        "def compute200thnearestneighbour(x, data): # Returns the distance of 200th nearest neighbour.\n",
        "    dists = []\n",
        "    for val in data:\n",
        "        dist = np.sum((x - val) **2 ) # computing distances.\n",
        "        if(len(dists) == 200 and dists[199] > dist):\n",
        "           # If distance is larger than current largest distance found.\n",
        "           \n",
        "          l = int(lower_bound(dists, dist)) # Using the lower bound function to get the right position.\n",
        "          if l < 200 and l >= 0 and dists[l] > dist:\n",
        "                dists[l] = dist\n",
        "        else:\n",
        "            dists.append(dist)\n",
        "            dists.sort()\n",
        "    \n",
        "    return dists[199] # Dist 199 contains the distance of 200th nearest neighbour.\n",
        "\n",
        "# Computing the 200th nearest neighbour distance of some point the dataset:\n",
        "twohundrethneigh = []\n",
        "for val in sent_vectors[:2000]:\n",
        "    twohundrethneigh.append( compute200thnearestneighbour(val, sent_vectors[:2000]) )\n",
        "twohundrethneigh.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIuQAaX3nlD9"
      },
      "source": [
        "# Plotting for the Elbow Method :\n",
        "plt.figure(figsize=(14,4))\n",
        "plt.title(\"Elbow Method for Finding the right Eps hyperparameter\")\n",
        "plt.plot([x for x in range(len(twohundrethneigh))], twohundrethneigh)\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.ylabel(\"Distance of 200th Nearest Neighbour\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MI4hkkEnl64"
      },
      "source": [
        "# Training DBSCAN :\n",
        "from sklearn.cluster import DBSCAN\n",
        "model = DBSCAN(eps = 5, min_samples = minPts, n_jobs=-1)\n",
        "model.fit(sent_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O88lJJKWnrVv"
      },
      "source": [
        "dfdb = df\n",
        "dfdb['AVG-W2V Clus Label'] = model.labels_\n",
        "dfdb.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGOHU2uGnwqF"
      },
      "source": [
        "dfdb.groupby(['AVG-W2V Clus Label'])['Reviews'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqdv3l-ZlvSC"
      },
      "source": [
        "Clustering Hierarchical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD6t5-6MlzpS"
      },
      "source": [
        "import scipy\n",
        "from scipy.cluster import hierarchy\n",
        "dendro=hierarchy.dendrogram(hierarchy.linkage(sent_vectors,method='ward'))\n",
        "plt.axhline(y=35)# cut at 30 to get 5 clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7rtziaNl6Hz"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  #took n=5 from dendrogram curve \n",
        "Agg=cluster.fit_predict(sent_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycmPDnM9l-H_"
      },
      "source": [
        "# Giving Labels/assigning a cluster to each point/text \n",
        "aggdfa = dfdb\n",
        "aggdfa['AVG-W2V Clus Label'] = cluster.labels_\n",
        "aggdfa.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYvxH1OsmB2K"
      },
      "source": [
        "# How many points belong to each cluster ->\n",
        "aggdfa.groupby(['AVG-W2V Clus Label'])['Reviews'].count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiBqzkGwmGAb"
      },
      "source": [
        "# Reading a review which belong to each group.\n",
        "for i in range(5):\n",
        "    print(\"2 reviews assigned to cluster \", i)\n",
        "    print(\"-\" * 70)\n",
        "    print(aggdfa.iloc[aggdfa.groupby(['AVG-W2V Clus Label']).groups[i][0]]['Reviews'])\n",
        "    print('\\n')\n",
        "    print(aggdfa.iloc[aggdfa.groupby(['AVG-W2V Clus Label']).groups[i][1]]['Reviews'])\n",
        "    print('\\n')\n",
        "    print(\"_\" * 70)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDMtMNAfemJy"
      },
      "source": [
        "In one paragraph, please compare K means, DBSCAN and Hierarchical clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoZDU4xnemJz"
      },
      "source": [
        "#You can write you answer here. (No code needed)\n",
        "\n",
        "\"\"\"the K-means algorithm using bag of words performed really well\n",
        "for the given dataset. the dataset was huge consisting of nearly\n",
        "more than 400 thousand records. because the size of data is large \n",
        "the computational time is taking too long execute. in terms of \n",
        "creating clusters k-means has been the best and the top terms in \n",
        "each cluster were generated accurately. the DBSCAN performance \n",
        "was not so good as most of the reviews were grouped to a particular \n",
        "cluster. the hierarchial clustering did well in grouping and dividing \n",
        "the data evenly. however in analysing the kinds of reviews clustered \n",
        "together k-means was the best of all. \"\"\" \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}