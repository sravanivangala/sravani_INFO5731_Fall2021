{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravanivangala/sravani_INFO5731_Fall2021/blob/main/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(20 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect all User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the abstracts research papers by using the query [smart health](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(4) Collect the top 10,000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter with the following requirements: Location (Texas), Time frame (2021-01-01 to 2021-09-01). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points) Here is a [legal case](https://github.com/unt-iialab/info5731-fall2021/blob/main/assignments/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "(1) Basic feature extraction using text data\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "(2) Basic Text Pre-processing of text data\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "(3) Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above.\n",
        "\n",
        "\n",
        "(4) Advance Text Processing\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "vATjQNTY8buA",
        "outputId": "f31238ab-ee97-4518-e300-d3eaf9f8a7d8"
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "#getting the data from url\n",
        "data_url = \"https://raw.githubusercontent.com/unt-iialab/info5731-fall2021/main/assignments/01-05-1%20%20Adams%20v%20Tanner.txt\"\n",
        "#reading the data \n",
        "train = pd.read_table(data_url, delimiter = \"\\t\", header=None)\n",
        "#counting number of words in each sentence\n",
        "train['word_count'] = train[0].apply(lambda x: len(str(x).split(\" \")))\n",
        "train[['word_count']].head()\n",
        "\n"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   word_count\n",
              "0           3\n",
              "1           4\n",
              "2           1\n",
              "3           1\n",
              "4           3"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9Zyu8PKqBWd",
        "outputId": "282befdf-27fe-4cdd-c778-31cd9bce0ac3"
      },
      "source": [
        "#counting number of sentences in the data\n",
        "train['sentence_count'] = train[0].apply(lambda x: len(str(x).split(\"\\n\")))\n",
        "#train[['word_count']].head()\n",
        "print(len(train['sentence_count']))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "dLYSGdlksAlr",
        "outputId": "45d2d7f9-9be2-4ad2-82cb-16acec72cbab"
      },
      "source": [
        "#counting number of characters in each sentence\n",
        "train['char_count'] = train[0].str.len() #this also includes spaces\n",
        "train[['char_count']].head()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   char_count\n",
              "0          10\n",
              "1          25\n",
              "2           5\n",
              "3           2\n",
              "4          18"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "g1jjUbIDsvnC",
        "outputId": "46f69a85-7cef-471b-b936-04f9974687a0"
      },
      "source": [
        "def avg_word(sentence):\n",
        "  words = sentence.split()\n",
        "  try:\n",
        "    return (sum(len(word) for word in words)/len(words)) #returning the avg word length in each sentence\n",
        "  except ZeroDivisionError:\n",
        "    return(0)\n",
        "# calculating avg word length of each sentence\n",
        "train['avg_word'] = train[0].apply(lambda x: avg_word(x))\n",
        "train[['avg_word']].head()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>avg_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.333333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   avg_word\n",
              "0  2.666667\n",
              "1  5.500000\n",
              "2  5.000000\n",
              "3  2.000000\n",
              "4  5.333333"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "Gj6m09wYuPrV",
        "outputId": "39fe6024-7c27-4b94-b6df-61e44c64c6b7"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "#counting number of stopwords in each sentence\n",
        "train['stopwords'] = train[0].apply(lambda x: len([x for x in x.split() if x in stop]))\n",
        "train[['stopwords']].head()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   stopwords\n",
              "0          0\n",
              "1          1\n",
              "2          0\n",
              "3          0\n",
              "4          0"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "PcAYesxGvxsX",
        "outputId": "51440db8-e3f4-487e-b5d0-befb23b6e730"
      },
      "source": [
        "#counting number of words starting with ** in each sentence\n",
        "train['imp_statements'] = train[0].apply(lambda x: len([x for x in x.split() if x.startswith('**')]))\n",
        "train[['imp_statements']].head()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>imp_statements</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   imp_statements\n",
              "0               0\n",
              "1               0\n",
              "2               0\n",
              "3               0\n",
              "4               0"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "3Yq59vVfwwUO",
        "outputId": "c288f62b-9c7a-491c-b7bb-6ed77d0d0061"
      },
      "source": [
        "#counting number of numerics in each sentence\n",
        "train['numerics'] = train[0].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
        "train[['numerics']].head()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>numerics</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   numerics\n",
              "0         2\n",
              "1         0\n",
              "2         0\n",
              "3         0\n",
              "4         0"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "m2qyZ6eYxBTM",
        "outputId": "b11715a6-f2cc-4dba-e4f6-e9fd4aa97f19"
      },
      "source": [
        "#counting number of upper case words in each sentence\n",
        "train['upper'] = train[0].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n",
        "train[['upper']].head()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>upper</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   upper\n",
              "0      0\n",
              "1      0\n",
              "2      1\n",
              "3      0\n",
              "4      3"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vMmtbf0ysIU",
        "outputId": "ae76303b-33fb-4e8c-84f4-828b23be734f"
      },
      "source": [
        "#converting all the sentences to lower case\n",
        "train['sentences'] = train[0].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "train['sentences'].head()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   5 ala. 740\n",
              "1    supreme court of alabama.\n",
              "2                        adams\n",
              "3                           v.\n",
              "4           tanner and horton.\n",
              "Name: sentences, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xCN_3kFzMgp",
        "outputId": "b14053c9-e78d-4c44-f49e-47aff16f9042"
      },
      "source": [
        "#removing the punctuations\n",
        "train['sentences'] = train['sentences'].str.replace('[^\\w\\s]','')\n",
        "train['sentences'].head()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                   5 ala 740\n",
              "1    supreme court of alabama\n",
              "2                       adams\n",
              "3                           v\n",
              "4           tanner and horton\n",
              "Name: sentences, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYrDgugKzhDR",
        "outputId": "e6faee64-9fef-4eae-89e5-50d70d5ad6a2"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "#removing the stop words\n",
        "train['sentences'] = train['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "train['sentences'].head()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                5 ala 740\n",
              "1    supreme court alabama\n",
              "2                    adams\n",
              "3                        v\n",
              "4            tanner horton\n",
              "Name: sentences, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78RK1RlJz7mP",
        "outputId": "2f6c24e5-2938-4d73-fbb3-abdd28b4f56d"
      },
      "source": [
        "#finding the frequently occuring words\n",
        "freq = pd.Series(' '.join(train['sentences']).split()).value_counts()[:10]\n",
        "freq"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "execution    50\n",
              "crop         49\n",
              "lien         25\n",
              "levy         25\n",
              "v            22\n",
              "claimants    22\n",
              "right        21\n",
              "case         21\n",
              "court        20\n",
              "gathered     19\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tR-nK4Ss0Kz4",
        "outputId": "0b6a25c4-ba33-450f-b6c5-2fbe58e83635"
      },
      "source": [
        "#removing the frequently occurring words \n",
        "freq = list(freq.index)\n",
        "train['sentences'] = train['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "train['sentences'].head()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          5 ala 740\n",
              "1    supreme alabama\n",
              "2              adams\n",
              "3                   \n",
              "4      tanner horton\n",
              "Name: sentences, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVOIuNGN0Zz6",
        "outputId": "954262fc-c862-49b4-9447-f440094b9f3c"
      },
      "source": [
        "#finding out the rare words\n",
        "freq = pd.Series(' '.join(train['sentences']).split()).value_counts()[-10:]\n",
        "freq"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "assert        1\n",
              "charged       1\n",
              "references    1\n",
              "alston        1\n",
              "create        1\n",
              "firm          1\n",
              "thus          1\n",
              "relieved      1\n",
              "remains       1\n",
              "464           1\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4apJq990kTx",
        "outputId": "870f4f63-587c-4cc9-8462-6edf1ad78bc0"
      },
      "source": [
        "#removing the rare words\n",
        "freq = list(freq.index)\n",
        "train['sentences'] = train['sentences'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "train['sentences'].head()"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          5 ala 740\n",
              "1    supreme alabama\n",
              "2              adams\n",
              "3                   \n",
              "4      tanner horton\n",
              "Name: sentences, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fIYOknZ0zRM",
        "outputId": "a7db2e21-448b-4088-84ce-993cebf151df"
      },
      "source": [
        "from textblob import TextBlob\n",
        "#spelling correction\n",
        "train['sentences'][:5].apply(lambda x: str(TextBlob(x).correct()))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          5 all 740\n",
              "1    supreme alabama\n",
              "2              adams\n",
              "3                   \n",
              "4      manner norton\n",
              "Name: sentences, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e85qiEKP1Eq-",
        "outputId": "01a5f0e5-d718-4446-d6e0-4d056228ed3e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#tokenization of sentence 18\n",
        "TextBlob(train['sentences'][18]).words"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['st1821', 'prohibiting', 'attaches', 'favor', 'fi', 'fa', 'growing', 'attach'])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3aQo59x1sU4",
        "outputId": "fa9ebb35-dfe1-47ec-ef96-1b86d5d36ec1"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "#stemming all the sentences\n",
        "train['sentences'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         5 ala 740\n",
              "1    suprem alabama\n",
              "2              adam\n",
              "3                  \n",
              "4     tanner horton\n",
              "Name: sentences, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtzHe81I3Q-S",
        "outputId": "0a5424cf-4a70-4faf-98de-5999652cedf8"
      },
      "source": [
        "from textblob import Word\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "#lemmatizing all the sentences\n",
        "train['sentences'] = train['sentences'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "train['sentences'].head()"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0          5 ala 740\n",
              "1    supreme alabama\n",
              "2               adam\n",
              "3                   \n",
              "4      tanner horton\n",
              "Name: sentences, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTfiK4_gy_YT"
      },
      "source": [
        "#saving all the sentences into a csv file\n",
        "\n",
        "import csv\n",
        "\n",
        "with open('legalcase_cleaned_data', 'w') as myfile:\n",
        "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
        "    x = train['sentences']\n",
        "    wr.writerow(x)"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "0EBmJGhx3rLu",
        "outputId": "c5b9893e-0d13-427a-f596-bd94bcdfb00a"
      },
      "source": [
        "#counting term frequency\n",
        "tf1 = (train['sentences'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
        "tf1.columns = ['words','tf']\n",
        "tf1"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>tf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alabama</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>supreme</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     words  tf\n",
              "0  alabama   1\n",
              "1  supreme   1"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX4i2QI-TQzZ",
        "outputId": "f6db128b-c36d-4023-9ffb-f82a643453a0"
      },
      "source": [
        "#NGram-2\n",
        "TextBlob(train['sentences'][1]).ngrams(2)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['supreme', 'alabama'])]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15vbWwX6UYWs",
        "outputId": "66a9f05c-dcfa-476e-9a40-0ef7d4c0801b"
      },
      "source": [
        "#NGram-1\n",
        "TextBlob(train['sentences'][1]).ngrams(1)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[WordList(['supreme']), WordList(['alabama'])]"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdcIPOT-Urh-",
        "outputId": "7e723373-429e-473a-f1da-b2a25aec1077"
      },
      "source": [
        "#NGram-3\n",
        "TextBlob(train['sentences'][1]).ngrams(3)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQKnPjPDHJHr"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "for i in range(148):\n",
        "\n",
        "    #  Using a Tagger. Which is part-of-speech \n",
        "    # tagger or POS-tagger. \n",
        "\n",
        "  wordsList=TextBlob(train['sentences'][i]).words #assigning each sentence from the document\n",
        "  tagged = nltk.pos_tag(wordsList)\n",
        "  \n",
        "  print(tagged)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsmILvaXg584",
        "outputId": "fa05dfe0-aab5-4630-8e96-61ea88c28d71"
      },
      "source": [
        "#creating virtual display\n",
        "!apt-get install -y xvfb # Install X Virtual Frame Buffer\n",
        "import os\n",
        "os.system('Xvfb :1 -screen 0 1600x1200x16  &')    # create virtual display with size 1600x1200 and 16 bit color\n",
        "os.environ['DISPLAY']=':1.0'    #to use our virtual DISPLAY :1.0."
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdub1iiNg_PV",
        "outputId": "d9bfa737-095b-4ff0-e8ba-dd062c56f9a2"
      },
      "source": [
        "#to display the nltk tree\n",
        "%matplotlib inline\n",
        "### INSTALL GHOSTSCRIPT (Required to display NLTK trees) ###\n",
        "!apt install ghostscript python3-tk"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ghostscript is already the newest version (9.26~dfsg+0-0ubuntu0.18.04.14).\n",
            "python3-tk is already the newest version (3.6.9-1~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "xv9gKpTZZao5",
        "outputId": "e9a31beb-1a5f-45b2-f016-c4f8180a75a8"
      },
      "source": [
        "#dependency parse tree\n",
        "import spacy\n",
        "from nltk.tree import Tree\n",
        "\n",
        "spacy_nlp = spacy.load(\"en\")\n",
        "\n",
        "sent = train['sentences'][18] #assigning a sentence from the document\n",
        "\n",
        "def nltk_spacy_tree(sent):\n",
        "   \n",
        "    doc = spacy_nlp(sent)\n",
        "    def token_format(token):\n",
        "        return \"_\".join([token.orth_, token.tag_, token.dep_]) #returning the token format\n",
        "\n",
        "    def to_nltk_tree(node):\n",
        "        if node.n_lefts + node.n_rights > 0: #checking if the tokens have nodes and returning a tree\n",
        "            return Tree(token_format(node),\n",
        "                       [to_nltk_tree(child) \n",
        "                        for child in node.children]\n",
        "                   )\n",
        "        else:\n",
        "            return token_format(node) #if the token has no nodes the format of the token is returned\n",
        "\n",
        "    tree = [to_nltk_tree(sent.root) for sent in doc.sents]\n",
        "    # The first item in the list is the full tree\n",
        "    display(tree[0])\n",
        "\n",
        "nltk_spacy_tree(sent) #calling the function"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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",
            "text/plain": [
              "Tree('prohibiting_NNP_ROOT', ['st1821_NNP_nsubj', Tree('attache_NNP_advmod', ['favor_NN_intj']), Tree('attach_NN_dobj', ['fi_NNP_compound', 'fa_FW_nmod', 'growing_VBG_amod'])])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16cMUGzRhoXV"
      },
      "source": [
        ""
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFkL1AfTg32K"
      },
      "source": [
        ""
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1pEL_ikYYeI",
        "outputId": "aae468c0-44ed-4ce0-edac-cc4875ebfa12"
      },
      "source": [
        "#3 Name Entity Recognition\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "for i in range(148):  \n",
        "  doc = nlp(train['sentences'][i]) #taking each sentence from the text document\n",
        "  \n",
        "  for ent in doc.ents:\n",
        "      print(ent.text, ent.start_char, ent.end_char, ent.label_) #printing the start and end position, label of the entity"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 0 1 CARDINAL\n",
            "740 6 9 CARDINAL\n",
            "supreme alabama 0 15 ORG\n",
            "tanner horton 0 13 PERSON\n",
            "june term 1843 0 14 DATE\n",
            "writ error circuit 0 18 ORG\n",
            "west headnotes 2 0 16 ORG\n",
            "1 0 1 CARDINAL\n",
            "chattel 0 7 ORG\n",
            "4 0 1 CARDINAL\n",
            "2 0 1 CARDINAL\n",
            "5 0 1 CARDINAL\n",
            "1 0 1 CARDINAL\n",
            "november 1840 25 38 DATE\n",
            "allen harrison 119 133 PERSON\n",
            "thirty 211 217 CARDINAL\n",
            "allen harrison 239 253 PERSON\n",
            "october 1839 426 438 DATE\n",
            "741 439 442 CARDINAL\n",
            "thereon 7th 450 461 ORG\n",
            "may 1840 652 660 DATE\n",
            "indorser burton harrison 684 708 PERSON\n",
            "fourteen thousand dollar 749 773 MONEY\n",
            "one hundred twenty acre 817 840 QUANTITY\n",
            "allen harrison 843 857 PERSON\n",
            "tennessee 963 972 GPE\n",
            "first 981 986 ORDINAL\n",
            "september 1840 987 1001 DATE\n",
            "three four 1011 1021 CARDINAL\n",
            "harrison 39 47 ORG\n",
            "first 158 163 ORDINAL\n",
            "may 1840 63 71 DATE\n",
            "2 109 110 CARDINAL\n",
            "1 148 149 CARDINAL\n",
            "361 159 162 CARDINAL\n",
            "307 175 178 CARDINAL\n",
            "604 190 193 CARDINAL\n",
            "1 2 199 202 PRODUCT\n",
            "418 212 215 CARDINAL\n",
            "41 249 251 CARDINAL\n",
            "167 254 257 CARDINAL\n",
            "742 266 269 CARDINAL\n",
            "3 352 353 CARDINAL\n",
            "112 470 473 CARDINAL\n",
            "john rep 480 488 PERSON\n",
            "338 489 492 CARDINAL\n",
            "5 497 498 CARDINAL\n",
            "26 508 510 CARDINAL\n",
            "john rep 514 522 PERSON\n",
            "235 523 526 CARDINAL\n",
            "8 527 528 CARDINAL\n",
            "693 538 541 CARDINAL\n",
            "4 647 648 CARDINAL\n",
            "w murphy w g jones 0 18 PERSON\n",
            "1821 38 42 DATE\n",
            "167 51 54 CARDINAL\n",
            "2 0 1 CARDINAL\n",
            "2 103 104 CARDINAL\n",
            "john rep 105 113 PERSON\n",
            "216 114 117 CARDINAL\n",
            "3 118 119 CARDINAL\n",
            "66 133 135 CARDINAL\n",
            "130 142 145 CARDINAL\n",
            "29 216 218 CARDINAL\n",
            "2 224 225 CARDINAL\n",
            "2 308 309 CARDINAL\n",
            "john rep 310 318 PERSON\n",
            "422 319 322 CARDINAL\n",
            "stewart 323 330 PERSON\n",
            "9 339 340 CARDINAL\n",
            "john rep 341 349 PERSON\n",
            "112 350 353 CARDINAL\n",
            "9 372 373 CARDINAL\n",
            "39 382 384 CARDINAL\n",
            "fourteen thousand dollar 562 586 MONEY\n",
            "year 53 57 DATE\n",
            "1840 266 270 DATE\n",
            "744 456 459 CARDINAL\n",
            "elliott mayfield 618 634 PERSON\n",
            "5 635 636 CARDINAL\n",
            "182 648 651 CARDINAL\n",
            "3 0 1 CARDINAL\n",
            "dane remarking 133 147 PERSON\n",
            "american 163 171 NORP\n",
            "supra 390 395 PERSON\n",
            "368 1 413 418 QUANTITY\n",
            "397 425 428 CARDINAL\n",
            "604 436 439 CARDINAL\n",
            "1821 651 655 DATE\n",
            "167 804 807 CARDINAL\n",
            "745 1040 1043 CARDINAL\n",
            "hurtell president c bank united state 1175 1212 ORG\n",
            "assignee citation 1213 1230 PERSON\n",
            "wood gary 1262 1271 PERSON\n",
            "4 0 1 CARDINAL\n",
            "746 102 105 CARDINAL\n",
            "4 0 1 CARDINAL\n",
            "210 46 114 120 CARDINAL\n",
            "non sequitur mischief statute 353 382 ORG\n",
            "747 502 505 CARDINAL\n",
            "5 0 1 CARDINAL\n",
            "anomaly legislature protection 264 294 ORG\n",
            "forbidden plaintiff 302 321 ORG\n",
            "5 0 1 CARDINAL\n",
            "740 6 9 CARDINAL\n",
            "1843 10 14 DATE\n",
            "284 18 21 CARDINAL\n",
            "2019 0 4 DATE\n",
            "thomson reuters 5 20 ORG\n",
            "9 7 8 CARDINAL\n",
            "1 0 1 CARDINAL\n",
            "55 0 2 CARDINAL\n",
            "266 7 10 CARDINAL\n",
            "271 11 14 CARDINAL\n",
            "hon j saffold 57 70 PERSON\n",
            "dec term 1876 0 13 PERSON\n",
            "2 0 1 CARDINAL\n",
            "co marshall 14 25 PERSON\n",
            "47 0 2 CARDINAL\n",
            "ala 3 6 ORG\n",
            "376 11 14 CARDINAL\n",
            "hon john cunningham 54 73 PERSON\n",
            "3 0 1 CARDINAL\n",
            "45 0 2 CARDINAL\n",
            "334 11 14 CARDINAL\n",
            "hon john cunningham 63 82 PERSON\n",
            "4 0 1 CARDINAL\n",
            "31 0 2 CARDINAL\n",
            "526 7 10 CARDINAL\n",
            "hon hale 51 59 PERSON\n",
            "5 0 1 CARDINAL\n",
            "21 0 2 CARDINAL\n",
            "error circuit 0 13 ORG\n",
            "hon b moore 28 39 PERSON\n",
            "jun term 1852 0 13 PERSON\n",
            "6 0 1 CARDINAL\n",
            "8 0 1 CARDINAL\n",
            "jacob cohen 9 20 PERSON\n",
            "cohen 59 64 PERSON\n",
            "jul term 0 8 PERSON\n",
            "7 0 1 CARDINAL\n",
            "65 0 2 CARDINAL\n",
            "258 11 14 CARDINAL\n",
            "three 18 23 CARDINAL\n",
            "hon 64 67 ORG\n",
            "8 0 1 CARDINAL\n",
            "edward thompson 2 17 PERSON\n",
            "4 0 1 CARDINAL\n",
            "913 5 8 CARDINAL\n",
            "914 9 12 CARDINAL\n",
            "may 1887 0 8 DATE\n",
            "9 0 1 CARDINAL\n",
            "103 0 3 CARDINAL\n",
            "annual 41 47 DATE\n",
            "1936 0 4 CARDINAL\n",
            "3 16 17 CARDINAL\n",
            "1 0 1 CARDINAL\n",
            "9 0 1 CARDINAL\n",
            "39 6 8 CARDINAL\n",
            "2 0 1 CARDINAL\n",
            "2 0 1 CARDINAL\n",
            "5 0 1 CARDINAL\n",
            "182 7 10 CARDINAL\n",
            "1837 15 19 DATE\n",
            "writ error circuit tuskaloosa 0 29 ORG\n",
            "2 0 1 CARDINAL\n",
            "3 0 1 CARDINAL\n",
            "9 0 1 CARDINAL\n",
            "six year 11 19 DATE\n",
            "one half 44 52 CARDINAL\n",
            "farm year 81 90 DATE\n",
            "2 0 1 CARDINAL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "**Write your explanations of the constituency parsing tree and dependency parsing tree here (Question 3-2):** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KF-4UwFoVNc"
      },
      "source": [
        "# You explaination here\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKF_fnwsoVNc"
      },
      "source": [
        "# **Question 4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV3j5x9-oVNd"
      },
      "source": [
        "(20 points) Python Regular Expression.\n",
        "\n",
        "(1) Write a Python program to remove leading zeros from an IP address.\n",
        "\n",
        "ip = \"260.08.094.109\"\n",
        "\n",
        "\n",
        "(2) Write a Python Program to extract all the years from the following sentence.\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUXbKBrwoVNd",
        "outputId": "9c931552-5ddd-4055-ab31-471c3a79cc0a"
      },
      "source": [
        "# removing leading zeroes\n",
        "\n",
        "def removeleadingzeroes(ip):\n",
        "  #splitting the ip string using '.' as delimeter\n",
        "  #removing the leading zeroes by converting them into int and joining them\n",
        "  newip = \".\".join([str(int(j))for j in ip.split(\".\")])\n",
        "  #returning the new ip address\n",
        "  return newip\n",
        "\n",
        "ip = \"260.08.094.109\"\n",
        "#printing the new ip address\n",
        "print(removeleadingzeroes(ip))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "260.8.94.109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTixRHxeqERN",
        "outputId": "6c13724c-4084-4708-de0d-69ccc0296c67"
      },
      "source": [
        "data = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "\n",
        "list1 = []\n",
        "flag = 0\n",
        "s = \"\"\n",
        "\n",
        "for i in data:\n",
        "  # checking if the current word from the sentence is a year\n",
        "  if i=='2':\n",
        "    flag = 1\n",
        "  #checking if the word is not a complete digit and year\n",
        "  if not(i.isdigit()) and flag==1:\n",
        "    flag = 0\n",
        "  #checking if the word is a complete digit and year\n",
        "  if i.isdigit() and flag==1:\n",
        "    s+=i\n",
        "  #append the year to the list 's' \n",
        "  if s!=\"\" and i==\" \":\n",
        "    list1.append(int(s))\n",
        "    s=\"\"\n",
        "\n",
        "#printing the list\n",
        "print(list1)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2010, 2010, 2019]\n"
          ]
        }
      ]
    }
  ]
}